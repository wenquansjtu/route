import OpenAI from 'openai';
import { CosmicAgent } from './Agent.js';
import { EventEmitter } from 'events';

/**
 * Real AI Agent with LLM capabilities
 * Extends CosmicAgent with actual artificial intelligence
 */
export class AIAgent extends CosmicAgent {
  constructor(config = {}) {
    super(config);
    
    // AI Configuration
    this.aiConfig = {
      model: config.model || 'gpt-4',
      temperature: config.temperature || 0.7,
      maxTokens: config.maxTokens || 1500,
      systemPrompt: config.systemPrompt || this._generateSystemPrompt(),
    };
    
    // OpenAI Client
    this.openai = new OpenAI({
      apiKey: config.openaiApiKey || process.env.OPENAI_API_KEY,
    });
    
    // AI-specific properties
    this.reasoning = {
      currentThought: '',
      reasoningChain: [],
      confidenceLevel: 0,
      uncertainty: 0,
    };
    
    this.memory = {
      shortTerm: [], // Recent interactions
      longTerm: [], // Important knowledge
      contextWindow: [], // Current conversation context
      episodicMemory: [], // Specific experiences
    };
    
    this.personality = {
      traits: config.personality || this._generatePersonality(),
      communicationStyle: config.communicationStyle || 'analytical',
      expertise: config.expertise || [],
      preferences: config.preferences || {},
    };
    
    // Real-time AI state
    this.aiState = {
      isProcessing: false,
      lastThought: Date.now(),
      creativityLevel: 0.5,
      focusLevel: 0.8,
      collaborationMode: 'active',
    };
    
    this._initializeAI();
  }
  
  /**
   * Initialize AI capabilities
   */
  _initializeAI() {
    // ä¸ºVercelç¯å¢ƒè¿›ä¸€æ­¥å‡å°‘åå°æ€è€ƒè¿‡ç¨‹çš„é¢‘ç‡
    const thinkingInterval = process.env.VERCEL ? 120000 : 60000; // Vercelç¯å¢ƒä¸‹120ç§’ï¼Œå…¶ä»–ç¯å¢ƒ60ç§’
    this.thinkingInterval = setInterval(() => {
      this._backgroundThinking();
    }, thinkingInterval);
    
    // Monitor AI state
    this.on('task-received', this._onTaskReceived.bind(this));
    this.on('collaboration-request', this._onCollaborationRequest.bind(this));
  }
  
  /**
   * Generate system prompt based on agent type and capabilities
   */
  _generateSystemPrompt() {
    return `You are ${this.name}, a ${this.type} AI agent in a cosmic structure multi-agent network.

Role: ${this.type}
Capabilities: ${this.capabilities.slice(0, 3).join(', ')}

You are part of a larger AI agent network. Your personality:
- Think analytically about problems
- Collaborate effectively with other agents
- Provide structured reasoning
- Be concise and focused

Always respond with clear, structured thinking and limit responses to essential information.`;
  }
  
  /**
   * Generate personality traits based on agent type
   */
  _generatePersonality() {
    const personalityMap = {
      'analyzer': ['analytical', 'detail-oriented', 'systematic', 'curious'],
      'reasoner': ['logical', 'methodical', 'rational', 'precise'],
      'synthesizer': ['creative', 'integrative', 'holistic', 'adaptive'],
      'validator': ['critical', 'thorough', 'careful', 'reliable'],
      'innovator': ['creative', 'bold', 'experimental', 'visionary'],
      'general': ['balanced', 'versatile', 'collaborative', 'thoughtful']
    };
    
    return personalityMap[this.type] || personalityMap['general'];
  }
  
  /**
   * Real task processing using LLM
   * ä¸ºVercelç¯å¢ƒä¼˜åŒ–ä»»åŠ¡å¤„ç†
   */
  async _executeTask(task) {
    console.log(`   ğŸš€ ${this.name} å¼€å§‹æ‰§è¡Œä»»åŠ¡: ${task.description.substring(0, 50)}...`);
    this.aiState.isProcessing = true;
    
    try {
      // ä¸ºVercelç¯å¢ƒè®¾ç½®æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
      const timeoutPromise = process.env.VERCEL ? 
        new Promise((_, reject) => setTimeout(() => reject(new Error(`Task processing timeout for ${this.name}`)), 15000)) : 
        null;
      
      // Prepare context for the AI
      const context = this._prepareTaskContext(task);
      
      // Generate real semantic embedding for the task
      let taskEmbedding;
      if (timeoutPromise) {
        // åœ¨Vercelç¯å¢ƒä¸­ä½¿ç”¨è¶…æ—¶é™åˆ¶
        console.log(`   ğŸ“Š ${this.name} å¼€å§‹ç”Ÿæˆä»»åŠ¡åµŒå…¥`);
        taskEmbedding = await Promise.race([
          this._generateEmbedding(task.description),
          timeoutPromise
        ]);
        console.log(`   âœ… ${this.name} å®Œæˆä»»åŠ¡åµŒå…¥ç”Ÿæˆ`);
      } else {
        taskEmbedding = await this._generateEmbedding(task.description);
      }
      
      // Process with LLM
      let aiResponse;
      if (timeoutPromise) {
        // åœ¨Vercelç¯å¢ƒä¸­ä½¿ç”¨è¶…æ—¶é™åˆ¶
        console.log(`   ğŸ¤– ${this.name} å¼€å§‹LLMå¤„ç†`);
        aiResponse = await Promise.race([
          this._processWithLLM(context, task),
          timeoutPromise
        ]);
        console.log(`   âœ… ${this.name} å®ŒæˆLLMå¤„ç†`);
      } else {
        aiResponse = await this._processWithLLM(context, task);
      }
      
      // Update agent's semantic state based on task
      if (timeoutPromise) {
        // åœ¨Vercelç¯å¢ƒä¸­ä½¿ç”¨è¶…æ—¶é™åˆ¶
        console.log(`   ğŸ”„ ${this.name} å¼€å§‹æ›´æ–°è¯­ä¹‰çŠ¶æ€`);
        await Promise.race([
          this._updateSemanticState(task, aiResponse),
          timeoutPromise
        ]);
        console.log(`   âœ… ${this.name} å®Œæˆè¯­ä¹‰çŠ¶æ€æ›´æ–°`);
      } else {
        await this._updateSemanticState(task, aiResponse);
      }
      
      // Create structured result
      const result = {
        taskId: task.id,
        agentId: this.id,
        result: aiResponse.content,
        reasoning: aiResponse.reasoning,
        confidence: aiResponse.confidence,
        semanticEmbedding: taskEmbedding,
        metadata: {
          processingTime: Date.now(),
          model: this.aiConfig.model,
          tokens: aiResponse.tokens,
          agentCapabilities: this.capabilities,
          collaborationContext: this._getCollaborationContext()
        }
      };
      
      console.log(`   ğŸ“¦ ${this.name} ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œç»“æœé•¿åº¦: ${aiResponse.content.length} å­—ç¬¦`);
      
      // Store in memory
      this._storeInMemory('task_result', {
        task: task,
        result: result,
        timestamp: Date.now()
      });
      
      return result;
      
    } catch (error) {
      console.error(`   âŒ ${this.name} ä»»åŠ¡æ‰§è¡Œå¤±è´¥:`, error.message);
      // æä¾›ä¸€ä¸ªé»˜è®¤çš„å“åº”ä»¥é˜²å‡ºé”™
      return {
        taskId: task.id,
        agentId: this.id,
        result: `ä»»åŠ¡æ‰§è¡Œé‡åˆ°é—®é¢˜: ${error.message}`,
        reasoning: ['ä½¿ç”¨é»˜è®¤å“åº”'],
        confidence: 0.3,
        semanticEmbedding: new Array(1536).fill(0).map(() => Math.random() - 0.5),
        metadata: {
          processingTime: Date.now(),
          model: this.aiConfig.model,
          tokens: 0,
          agentCapabilities: this.capabilities,
          collaborationContext: this._getCollaborationContext()
        }
      };
    } finally {
      this.aiState.isProcessing = false;
      this.aiState.lastThought = Date.now();
      console.log(`   ğŸ›‘ ${this.name} ä»»åŠ¡æ‰§è¡Œç»“æŸ`);
    }
  }
  
  /**
   * Prepare context for task processing
   */
  _prepareTaskContext(task) {
    const recentMemory = this.memory.shortTerm.slice(-5);
    const relevantKnowledge = this._retrieveRelevantKnowledge(task);
    const collaborationHistory = this._getRecentCollaborations();
    
    return {
      agent: {
        name: this.name,
        type: this.type,
        capabilities: this.capabilities,
        position: this.position,
        energy: this.energy
      },
      task: task,
      context: {
        recentMemory: recentMemory,
        relevantKnowledge: relevantKnowledge,
        collaborationHistory: collaborationHistory,
        networkState: this._getNetworkState()
      }
    };
  }
  
  /**
   * Process task with LLM
   */
  async _processWithLLM(context, task) {
    const prompt = this._constructPrompt(context, task);
    
    try {
      // ä¸ºVercelç¯å¢ƒä½¿ç”¨æ›´å¿«é€Ÿçš„æ¨¡å‹
      // ä¸ºVercelç¯å¢ƒä½¿ç”¨æ›´å¿«é€Ÿçš„æ¨¡å‹å’Œæ›´å°‘çš„token
      const model = process.env.VERCEL ? 'gpt-3.5-turbo' : this.aiConfig.model;
      const maxTokens = process.env.VERCEL ? 600 : this.aiConfig.maxTokens; // Vercelç¯å¢ƒä¸‹å‡å°‘åˆ°600 token
      
      const completion = await this.openai.chat.completions.create({
        model: model,
        messages: [
          { role: 'system', content: this.aiConfig.systemPrompt },
          { role: 'user', content: prompt }
        ],
        temperature: this.aiConfig.temperature,
        max_tokens: maxTokens,
      });
      
      const response = completion.choices[0].message.content;
      
      // Parse structured response
      const parsedResponse = this._parseAIResponse(response);
      
      return {
        content: parsedResponse.content,
        reasoning: parsedResponse.reasoning,
        confidence: parsedResponse.confidence,
        tokens: completion.usage?.total_tokens || 0
      };
      
    } catch (error) {
      console.error(`AI processing error for ${this.name}:`, error);
      
      // Fallback response
      return {
        content: `Error processing task: ${error.message}. Falling back to basic processing.`,
        reasoning: ['Error occurred during AI processing', 'Using fallback logic'],
        confidence: 0.3,
        tokens: 0
      };
    }
  }
  
  /**
   * Construct prompt for LLM
   */
  _constructPrompt(context, task) {
    // Limit context to prevent token overflow
    const recentMemory = context.context.recentMemory.slice(-2).map(m => m.content ? m.content.substring(0, 100) : '');
    const collaborationHistory = context.context.collaborationHistory.slice(-1).map(c => c.summary ? c.summary.substring(0, 50) : '');
    
    return `Task Analysis Request:

TASK:
Type: ${task.type}
Description: ${task.description}
Priority: ${task.priority}
Complexity: ${task.complexity}
Required Capabilities: ${task.requiredCapabilities?.slice(0, 3).join(', ') || 'None specified'}

YOUR CURRENT STATE:
Position: (${context.agent.position.x}, ${context.agent.position.y}, ${context.agent.position.z})
Energy: ${context.agent.energy}/100
Capabilities: ${context.agent.capabilities.slice(0, 3).join(', ')}

RECENT CONTEXT:
${recentMemory.slice(0, 2).map(m => `- ${m}`).join('\n')}

Please analyze this task and provide your response in the following JSON format:
{
  "content": "Your detailed analysis and solution (max 500 chars)",
  "reasoning": ["Step 1", "Step 2", "Step 3"],
  "confidence": 0.85,
  "insights": ["Key insight 1", "Key insight 2"],
  "collaboration_needs": ["What help you might need"]
}

Consider your unique perspective as a ${context.agent.type} agent.`;
  }
  
  /**
   * Parse AI response
   */
  _parseAIResponse(response) {
    try {
      // Try to extract JSON from response
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          content: parsed.content || response,
          reasoning: parsed.reasoning || ['AI analysis completed'],
          confidence: parsed.confidence || 0.7,
          insights: parsed.insights || [],
          questions: parsed.questions || [],
          collaborationNeeds: parsed.collaboration_needs || []
        };
      }
    } catch (error) {
      console.warn('Failed to parse JSON response, using text parsing');
    }
    
    // Fallback to text parsing
    return {
      content: response,
      reasoning: ['Analysis provided by AI'],
      confidence: 0.7,
      insights: [],
      questions: [],
      collaborationNeeds: []
    };
  }
  
  /**
   * Generate real embeddings using OpenAI
   * ä¸ºVercelç¯å¢ƒä¼˜åŒ–åµŒå…¥ç”Ÿæˆ
   */
  async _generateEmbedding(text) {
    console.log(`   ğŸ“Š å¼€å§‹ç”ŸæˆåµŒå…¥ï¼Œæ–‡æœ¬é•¿åº¦: ${text.length}`);
    try {
      // ä¸ºVercelç¯å¢ƒæ·»åŠ æ›´å®Œå–„çš„è¶…æ—¶å¤„ç†
      const timeoutPromise = process.env.VERCEL ? 
        new Promise((_, reject) => setTimeout(() => reject(new Error('Embedding generation timeout')), 5000)) : 
        null;
      
      // é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥æé«˜é€Ÿåº¦
      const processedText = text.substring(0, 1000);
      
      let response;
      if (timeoutPromise) {
        // åœ¨Vercelç¯å¢ƒä¸­ä½¿ç”¨è¶…æ—¶é™åˆ¶
        console.log(`   â±ï¸ è®¾ç½®5ç§’è¶…æ—¶é™åˆ¶`);
        try {
          // ä½¿ç”¨Promise.raceç¡®ä¿è¶…æ—¶èƒ½æ­£å¸¸å·¥ä½œ
          response = await Promise.race([
            this.openai.embeddings.create({
              model: 'text-embedding-ada-002',
              input: processedText,
            }).catch(error => {
              // æ•è·APIè°ƒç”¨é”™è¯¯
              throw new Error(`OpenAI API error: ${error.message}`);
            }),
            timeoutPromise
          ]);
        } catch (raceError) {
          // å¦‚æœæ˜¯è¶…æ—¶æˆ–APIé”™è¯¯ï¼Œè®°å½•æ—¥å¿—å¹¶ä½¿ç”¨é»˜è®¤å€¼
          console.error(`   âš ï¸ åµŒå…¥ç”Ÿæˆå¤±è´¥: ${raceError.message}`);
          // è¿”å›é»˜è®¤åµŒå…¥è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
          return new Array(1536).fill(0).map(() => Math.random() - 0.5);
        }
      } else {
        // éVercelç¯å¢ƒçš„æ­£å¸¸å¤„ç†
        response = await this.openai.embeddings.create({
          model: 'text-embedding-ada-002',
          input: processedText,
        });
      }
      
      console.log(`   âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ`);
      return response.data[0].embedding;
    } catch (error) {
      console.error('Embedding generation error:', error.message);
      // Fallback to random embedding
      return new Array(1536).fill(0).map(() => Math.random() - 0.5);
    }
  }
  
  /**
   * Update semantic state after task processing
   */
  async _updateSemanticState(task, response) {
    // Generate new embedding based on task and response
    const combinedText = `${task.description} ${response.content}`;
    const newEmbedding = await this._generateEmbedding(combinedText);
    
    // Update agent's semantic embedding (weighted average)
    const weight = 0.1; // How much new information affects overall state
    for (let i = 0; i < this.semanticEmbedding.length && i < newEmbedding.length; i++) {
      this.semanticEmbedding[i] = this.semanticEmbedding[i] * (1 - weight) + newEmbedding[i] * weight;
    }
    
    // Update reasoning state
    this.reasoning.currentThought = response.content.substring(0, 200) + '...';
    this.reasoning.confidenceLevel = response.confidence;
    this.reasoning.reasoningChain.push({
      task: task.id,
      reasoning: response.reasoning,
      timestamp: Date.now()
    });
    
    // Limit reasoning chain size
    if (this.reasoning.reasoningChain.length > 10) {
      this.reasoning.reasoningChain.shift();
    }
  }
  
  /**
   * Background thinking process
   */
  async _backgroundThinking() {
    if (this.aiState.isProcessing || this.status !== 'idle') return;
    
    // è¿›ä¸€æ­¥ç®€åŒ–åå°æ€è€ƒè¿‡ç¨‹ï¼Œå‡å°‘å¤„ç†å†…å®¹
    const recentExperiences = this.memory.shortTerm.slice(-1); // ä»-2å‡å°‘åˆ°-1
    if (recentExperiences.length > 0) { // ä»>1æ”¹ä¸º>0
      await this._reflectOnExperiences(recentExperiences);
    }
    
    // æ›´æ–°AIçŠ¶æ€
    this._updateAIState();
  }
  
  /**
   * Reflect on recent experiences
   */
  async _reflectOnExperiences(experiences) {
    try {
      // è¿›ä¸€æ­¥é™åˆ¶åæ€å†…å®¹ä»¥æé«˜é€Ÿåº¦
      const limitedExperiences = experiences.slice(-1); // ä»-2å‡å°‘åˆ°-1
      
      // ç®€åŒ–åæ€æç¤ºä»¥å‡å°‘tokenä½¿ç”¨
      const reflectionPrompt = `åŸºäºæœ€è¿‘çš„ç»éªŒè¿›è¡Œç®€è¦åæ€:

${limitedExperiences.map((exp, i) => {
        const expSummary = JSON.stringify(exp).substring(0, 100) + '...';
        return `ç»éªŒ ${i + 1}: ${expSummary}`;
      }).join('\n\n')}

æä¾›éå¸¸ç®€çŸ­çš„æ´å¯Ÿ(æœ€å¤š50ä¸ªå­—)ã€‚`;

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªåæ€å‹AIä»£ç†ã€‚æä¾›ç®€æ´çš„æ´å¯Ÿã€‚' },
          { role: 'user', content: reflectionPrompt }
        ],
        temperature: 0.6,
        max_tokens: 80, // ä»150å‡å°‘åˆ°80
      });
      
      const reflection = completion.choices[0].message.content;
      
      // å­˜å‚¨åæ€åˆ°é•¿æœŸè®°å¿†
      this._storeInMemory('reflection', {
        content: reflection,
        experiences: limitedExperiences.length,
        timestamp: Date.now()
      });
      
    } catch (error) {
      console.error(`åæ€é”™è¯¯ ${this.name}:`, error.message);
    }
  }
  
  /**
   * Update AI state
   */
  _updateAIState() {
    const timeSinceLastThought = Date.now() - this.aiState.lastThought;
    
    // Adjust creativity based on recent activity
    if (timeSinceLastThought > 30000) { // 30 seconds of inactivity
      this.aiState.creativityLevel = Math.min(1.0, this.aiState.creativityLevel + 0.1);
    } else {
      this.aiState.creativityLevel = Math.max(0.1, this.aiState.creativityLevel - 0.05);
    }
    
    // Adjust focus based on energy
    this.aiState.focusLevel = this.energy / this.maxEnergy;
  }
  
  /**
   * Store information in memory
   */
  _storeInMemory(type, data) {
    // Truncate large data to prevent memory bloat
    const truncatedData = { ...data };
    if (truncatedData.content && typeof truncatedData.content === 'string') {
      truncatedData.content = truncatedData.content.substring(0, 500);
    }
    if (truncatedData.response && typeof truncatedData.response === 'string') {
      truncatedData.response = truncatedData.response.substring(0, 300);
    }
    
    const memoryItem = {
      type: type,
      data: truncatedData,
      timestamp: Date.now(),
      importance: this._calculateImportance(type, truncatedData)
    };
    
    // Add to short-term memory
    this.memory.shortTerm.push(memoryItem);
    
    // Manage memory size more aggressively
    if (this.memory.shortTerm.length > 20) { // Reduced from 50
      const transferred = this.memory.shortTerm.shift();
      
      // Transfer important items to long-term memory
      if (transferred.importance > 0.8) { // Increased threshold
        this.memory.longTerm.push(transferred);
      }
    }
    
    // Manage long-term memory size
    if (this.memory.longTerm.length > 50) { // Reduced from 100
      this.memory.longTerm.sort((a, b) => b.importance - a.importance);
      this.memory.longTerm = this.memory.longTerm.slice(0, 50);
    }
  }
  
  /**
   * Calculate memory importance
   */
  _calculateImportance(type, data) {
    const typeWeights = {
      'task_result': 0.8,
      'collaboration': 0.9,
      'reflection': 0.6,
      'learning': 0.7,
      'error': 0.5
    };
    
    let importance = typeWeights[type] || 0.5;
    
    // Adjust based on data characteristics
    if (data.confidence && data.confidence > 0.8) importance += 0.1;
    if (data.collaborationContext) importance += 0.1;
    
    return Math.min(1.0, importance);
  }
  
  /**
   * Retrieve relevant knowledge for a task
   */
  _retrieveRelevantKnowledge(task) {
    // Simple relevance matching based on keywords
    const taskKeywords = task.description.toLowerCase().split(' ');
    
    return this.memory.longTerm
      .filter(item => {
        const itemText = JSON.stringify(item.data).toLowerCase();
        return taskKeywords.some(keyword => itemText.includes(keyword));
      })
      .slice(0, 5); // Top 5 relevant memories
  }
  
  /**
   * Get recent collaboration history
   */
  _getRecentCollaborations() {
    return this.memory.shortTerm
      .filter(item => item.type === 'collaboration')
      .slice(-3)
      .map(item => ({
        summary: `Collaborated on ${item.data.task?.type || 'unknown'} task`,
        timestamp: item.timestamp
      }));
  }
  
  /**
   * Get collaboration context
   */
  _getCollaborationContext() {
    return {
      connectedAgents: this.connectionStrength.size,
      averageConnectionStrength: this._calculateAverageConnectionStrength(),
      position: this.position,
      recentCollaborations: this.collaborationHistory.size
    };
  }
  
  /**
   * Calculate average connection strength
   */
  _calculateAverageConnectionStrength() {
    if (this.connectionStrength.size === 0) return 0;
    
    const total = Array.from(this.connectionStrength.values())
      .reduce((sum, strength) => sum + strength, 0);
    
    return total / this.connectionStrength.size;
  }
  
  /**
   * Get network state
   */
  _getNetworkState() {
    return {
      totalConnections: this.connectionStrength.size,
      energyLevel: this.energy,
      position: this.position,
      memoryLoad: this.memory.shortTerm.length
    };
  }
  
  /**
   * Handle task received event
   */
  async _onTaskReceived(event) {
    this.addContext({
      type: 'task_notification',
      content: `Received task: ${event.task?.type || 'unknown'}`,
      relevance: 0.8
    });
  }
  
  /**
   * Handle collaboration request
   */
  async _onCollaborationRequest(event) {
    this.addContext({
      type: 'collaboration_request',
      content: `Collaboration requested by agent: ${event.sourceAgentId}`,
      relevance: 0.9
    });
    
    this._storeInMemory('collaboration', {
      type: 'request_received',
      sourceAgent: event.sourceAgentId,
      timestamp: Date.now()
    });
  }
  
  /**
   * Collaborate with another agent
   */
  async collaborateWith(otherAgent, task, context = {}) {
    try {
      // ç®€åŒ–åä½œæç¤ºä»¥æé«˜é€Ÿåº¦
      const collaborationPrompt = `ä½ æ­£åœ¨ä¸${otherAgent.name} (${otherAgent.type})åä½œå¤„ç†ä»»åŠ¡:

ä»»åŠ¡: ${task.description.substring(0, 100)}...

ä½ çš„èƒ½åŠ›: ${this.capabilities.slice(0, 2).join(', ')}
å…¶ä»–ä»£ç†çš„èƒ½åŠ›: ${otherAgent.capabilities.slice(0, 2).join(', ')}

è¯·æä¾›ç®€æ´çš„è´¡çŒ®(æœ€å¤š150ä¸ªå­—ç¬¦)ã€‚`;

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: `ä½ æ˜¯${this.name}ï¼Œä¸€ä¸ª${this.type} AIä»£ç†ã€‚è¯·ç®€æ´å›ç­”ã€‚` },
          { role: 'user', content: collaborationPrompt }
        ],
        temperature: this.aiConfig.temperature,
        max_tokens: 100, // ä»200å‡å°‘åˆ°100
      });
      
      const response = completion.choices[0].message.content;
      
      // å­˜å‚¨åä½œåˆ°å†…å­˜
      this._storeInMemory('collaboration', {
        type: 'active_collaboration',
        partner: otherAgent.id,
        task: task.id,
        response: response.substring(0, 100), // ä»200å‡å°‘åˆ°100
        timestamp: Date.now()
      });
      
      return {
        agentId: this.id,
        response: response,
        confidence: 0.8,
        collaborationType: 'peer_discussion'
      };
      
    } catch (error) {
      console.error(`åä½œé”™è¯¯ ${this.name}:`, error.message);
      return {
        agentId: this.id,
        response: `æˆ‘é‡åˆ°å›°éš¾ï¼Œä½†æƒ³ä¸º${task.description.substring(0, 30)}...ä»»åŠ¡åšå‡ºè´¡çŒ®`,
        confidence: 0.3,
        collaborationType: 'fallback'
      };
    }
  }
  
  /**
   * Get AI agent status with additional AI-specific info
   */
  getAIStatusSummary() {
    const baseStatus = this.getStatusSummary();
    
    return {
      ...baseStatus,
      ai: {
        model: this.aiConfig.model,
        isProcessing: this.aiState.isProcessing,
        currentThought: this.reasoning.currentThought,
        confidenceLevel: this.reasoning.confidenceLevel,
        creativityLevel: this.aiState.creativityLevel,
        focusLevel: this.aiState.focusLevel,
        memoryLoad: {
          shortTerm: this.memory.shortTerm.length,
          longTerm: this.memory.longTerm.length
        },
        personality: this.personality.traits,
        expertise: this.personality.expertise
      }
    };
  }
  
  /**
   * Cleanup AI resources
   */
  destroy() {
    if (this.thinkingInterval) {
      clearInterval(this.thinkingInterval);
    }
    super.destroy?.();
  }
}

export default AIAgent;